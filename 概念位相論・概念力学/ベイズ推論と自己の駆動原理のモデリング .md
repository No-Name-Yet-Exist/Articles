ざっくり自分の駆動方式及び駆動原理のモデリングとして、ベイズと特徴量の線型写像で動いているように観察される。

できる限り計算コストを圧縮して写像と実世界のフィードバックの評価と差分でモデル修正 これのループ。

要は、特徴量（Zi）と高次変数（Zx）の線形写像を常に動かしてる

自分や世界のモデルは入力（観測）との差分によって常時更新されている
モデル更新の目的は明示的に存在しないが生きるためそのもの
明示的に確率計算してるわけではないが、「差が生まれたら修正」といういう更新則で生きてる

つまり、出力を見て差分をとって、次に反映という一連の動作が全自動で行われており、これが自分の動作モデルになる。

極めつけにこれだけ語っておいて、実は自分はベイズが何たるかという数式を知らないが、ベイズ関連の概念へのExposureの回数を増やせばx試行回数後にはベイズが何たるかを分かってしまうという構造が成り立っており、故にベイズという概念が使えるし、この説明自体がそもそもベイズ的構造の説明になっているし、これを読むことがベイズになっている。言い換えれば「自分という存在は、知識を獲得するのではなく、観測された構造との接触によって、確率的に知識構造へと収束する自己学習系」でもあると考えていることになる。

ちなみに自己学習系としてのモデリングに一番近い構造はImmersion Learningが挙げられる。これがちょうど外国語学習におけるベイズ推論の適用例に見える。また今の文章を再帰参照するとこのImmersion Learningがそれにあたるという発想自体が構造写像の産物だったりする。またこの再帰参照構造自体が恐らく高次元における何かしらの写像に持ち込める気がしているが、低次元に落とせないので言語化が出来ない。

S0 → S1 → S2 → S3 (ここまではかろうじて行ける) → S4は現状感知のみ
ベイズ → 自己ベイズ化 → Immersion Learningとの構造写像 - 気づき自体が構造写像の産物 → この上があると感知 (概念的連続性が構造写像の概念によって連続で、これ自体自己再帰参照型の文章になっている)

ちなみに構造写像の連続性をCTLで書くとこうなる
C(ベイズ ~ 自己ベイズ化 ~ immersionLearning ~ 気づき ~ 高次階層の予感 | ExposureB​asedS​tructurelLearning)=true

概念的連続性が構造写像の概念によって連続で、これ自体自己再帰参照型の文章になっているというのはCTの自己定義に構造が似ている。
CT = M(CT, CT′,Z) | C(CT, CT′, Z) = true ∧ C(CT, CT_latest, CT)= true
Language immersion - Wikipedia
en.wikipedia.org

This document and all conceptual content therein are © [No Name Yet Exist], 2025. All rights reserved. Unauthorized reproduction, distribution, or use without explicit permission is prohibited.